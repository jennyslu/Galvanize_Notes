# Morning Lecture

## Decision Boundaries

Let us consider a set of perfectly separable data. What will decision boundaries look like?

### Random Forests

Random forest is going to spare data into squares. It also doesn't know what to do with gaps.

### Logistic Regression

Must regularize (C=value hyper parameter) in order for logistic regression to work for perfectly separable dataset because βs will go to ∞ because there are many different possible lines. If there was overlap in the data it would be fine. Would still give us the slope we want though?

When the data is separable, there is an incentive for β^ to get bigger and bigger, to "emphasize" more and more the difference between the two classes.  

For example, if you double the size of beta, then elements in class 1 get bigger log odds, and elements in class 0 get smaller log odds.  So, from the perspective of minimizing the loss function, it is always better to make beta bigger and bigger.

In this way, beta won't converge.  But, the direction that beta points in will converge.  In particular, β^/|β^| converges.

Setting different thresholds will move line parallel-y.

# Afternoon Lecture

Softer margin is lower variance and higher bias (i.e. generalizes better)

The kernel is a thing that allows us to calculate the dot product of vectors in a transformed space without actually having to figure out what the transformation actually is. And since decision and optimization only depend on dot products of vectors, we can just put kernel into those and we never actually have to do transformation and solve our problem in higher space.

You can always separate data in infinite dimensional space.

Where did the margin go? It is in the higher dimensional space. When we come back to normal - we can no longer see it. Especially in case of 'rbf' kernel it is __very__ easy to overfit data.

## Gamma

As gamma increases = __over fitting__.

These Gaussians are over data points and if gamma is too large each point gets its own little Gaussian and it picks one or the other and circles each one.

## C

As C increases you care _more_ about getting stuff right. Try harder to get everything right = __over fitting__. But not as bad as increasing gamma. It doesn't individually circle every data point. 
