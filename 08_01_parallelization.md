# Morning Lecture

## S3

 - file storage

## EBS

 - hard drive attached to EC2 instance
 - can be connected directly to S3
 - probably/hopefully will not have to mess with it too much
 - you can create an additional EBS volume and mount it to EC2 instance
 -

## EC2

 - slice of computer and sell to you
 - instance is the name of the single machine you got
 - AMI allows you to load an OS you want. Can also save AMI if you want to keep what your work
 - regions:
  * availability zones: subsets of regions; often but not always specific data centres
  * in general, communication within an AZ is very fast
  * generally, cannot do things cross-region

### Snapshots

 - snapshots are saved automatically

### Volumes

 - paying for

## Key pairs

- in general, you do not want to use same key pair for multiple services
- SSH key pairs that were generated by EC2 for you
* __download and save this__

### Instances

 - terminated: not paying for in any way
  * everything dies
  * EBS does not necessarily die
  * by default, terminating will also kill EBS; you can uncouple and save EBS
 - active: paying for at hourly rate
 - stopped: not paying for compute time but you are paying for EBS volume

### EC2 tips

- get approximately as much ram as 4x your data
- __turn things off when you're not using them__
- __run everything through tmux__. Start tmus session for long code
- you pay __by hour__

## AWS CLI

 - Amazon wrote its own command line tool that you can install to interact with AWS
 - make sure you upgrade AWS CLI if it is already installed on your instance of EC2!!
 - __Credentials__
  * access keys/secret access keys
  * you will never be able to see secret access key again
  * _let's you do anything_
 - Various services will look for it in certain places
  * .aws/credentials
  * .boto/
  * can also put in bash profile

## Creating an instance

1. Choose AMI
 - go with Ubuntu for ease of use
2. Choose instance type
 - EBS only, memory optimized, etc.

## Connect to instance

1. Connect with ssh
 * `ssh -i ~/.ssh/key_pair_1.pem ubuntu@ec2-54-87-48-250.compute-1.amazonaws.com,54.87.48.250`
 * say yes if connecting for first time
2. We are now in terminal for this remote machine
 * LEARN VIM
3. Open tmux session
 * you are now in a sub terminal like session

```bash
which python
tmux
```

## SSH keys

SSH keys provide a more secure way of logging into a virtual private server with SSH than using a password alone. While a password can eventually be cracked with a brute force attack, SSH keys are nearly impossible to decipher by brute force alone. Generating a key pair provides you with two long string of characters: a public and a private key. You can place the public key on any server, and then unlock it by connecting to it with a client that already has the private key. When the two match up, the system unlocks without the need for a password. You can increase security even more by protecting the private key with a passphrase.

Fingerprints are created by applying a cryptographic hash function to a public key. Since fingerprints are shorter than the keys they refer to, they can be used to simplify certain key management tasks. In Microsoft software, "thumbprint" is used instead of "fingerprint."

```
KeyName        : ulynnejfirst
KeyFingerprint : 5b:a6:4e:78:11:75:46:51:38:93:dc:ff:69:...etc...
KeyMaterial    : -----BEGIN RSA PRIVATE KEY-----
                 super long string
                 -----END RSA PRIVATE KEY-----
```

# Afternoon Lecture

If we are bound by CPU:
 * parallelize: split up job and assign to each cores
 * __some jobs are easier to split than others__

If we are IO bound:
 * There are other ways to parallelize
 * web scraping
  - keep calling and as the different pages come in you use different resources

Python treats hyper-threaded things as separate cores

There are ways to parallelize matrix operations and numpy is doing it under the hood already.

Lots of bad things you can do (maxing out CPU, hard drive, RAM, etc.)

Create thread object that has target function. Iterates through list and starts all threads.


```python
import system

for t in threads:
    #goes through this loop instantly
    t.start()
for t in threads:
    #first thread might be last thread that finishes
    #doesn't matter because this loop says just wait
    #until all threads are finished
    #if you did not call this and the next line of code depended on results
    # would break
    t.join()

print() # super slow
## if you want to do logging
## use system
system.std.out()
```

```python

from sys import argv
from math import floor, sqrt
from threading import Thread


TOTAL_FACTORS = 0


def count_factors(number, start, end):
    # set global variable
    # use global list to fix?
    global TOTAL_FACTORS
    # each function will run same function
    # they will all update this global variable
    for i in xrange(start, end):
        if number % i == 0:
            TOTAL_FACTORS += 1


if __name__ == '__main__':

    number = int(argv[1])
    num_threads = int(argv[2])

    start = 2
    end = int(floor(sqrt(number))) + 1  # range is [start, end)

    num_to_test = end - start

    tests_per_thread = num_to_test / num_threads
    remainders = num_to_test % num_threads

    threads = []
    for i in xrange(num_threads):
        start_here = start + i * tests_per_thread
        end_here = start + (i+1) * tests_per_thread
        if i < remainders:
            start_here += i
            end_here += i+1
        else:
            start_here += remainders
            end_here += remainders
        threads.append(Thread(target=count_factors, args=(number, start_here, end_here)))

    for t in threads:
        t.start()

    for t in threads:
        t.join()

    print "# Factors:", TOTAL_FACTORS

```

## Race condition

Depending on how you set up code you can have same thing trying to access same data at inappropriate times

There is a very small time delay between when we access global variable `TOTAL_FACTORS` and when we increment it. It is possible that during that time it got updated by another thread.

This becomes more and more of a problem with more increasing numbers of threads

## Multiprocessing instead

Multiple processes typically can't share data so we have to use `Queue` from `multiprocessing`

Each process has separate copies of data so it is possible to overwhelm memory.

## I/O vs CPU bound

I/O bound typically easier to parallelize than CPU bound

If you really care about performance, you shouldn't use Python anyway (use Java/C).

In Python if we want to be faster can use numpy arrays which do stuff behind scenes for us.

One place where it is useful to do multi-threading and multi-processing in Python is web scraping.

## Limitations

Multi-threading will not help you if your process is truly bound by CPU. It will only be helpful if at some point you are waiting for something your system can jump to a different thread and do that.

Multi-processing can help you solve CPU bound problem. Take application and apply it to different cores.

## WHy is this useful

Recognize that there are problems which are parallelize-able.

Will get faster and faster as you split up resources but will get worse at some point because you are splitting up your resources too much. There is no point in trying to locate this minimum.

Can run out of memory if you try to split it up and data set was not that small because sklearn will just copy it all to the other instances of those processes.

Look around to see if someone else has already find a way to parallelize this process.
